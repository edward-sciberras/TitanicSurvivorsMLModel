{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the datasets\n",
    "training = pd.read_csv (r'datasets/train.csv')\n",
    "test = pd.read_csv (r'datasets/test.csv')\n",
    "full_sets = [training, test] # combining sets to process at same time\n",
    "submission = pd.DataFrame(full_sets[1][\"PassengerId\"]) # this will be used to submit predictions\n",
    "\n",
    "# removing unwanted columns\n",
    "for dataset in full_sets:\n",
    "#     dataset.drop(\"Embarked\", axis = 1, inplace = True) # removing embarked\n",
    "    dataset.drop(\"Cabin\", axis = 1, inplace = True) # removing cabin\n",
    "    dataset.drop(\"Ticket\", axis = 1, inplace = True) # removing ticket \n",
    "    dataset.drop(\"Name\", axis = 1, inplace = True) # removing name\n",
    "    dataset.drop(\"PassengerId\", axis = 1, inplace = True) # removing passenger index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoder\n",
    "# changing male to 1 and female to 0 \n",
    "for dataset in full_sets:\n",
    "    ageLabelEncoder = LabelEncoder()\n",
    "    dataset[\"Sex\"] = ageLabelEncoder.fit_transform(dataset[\"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Embarked as S, C, Q\n",
    "for dataset in full_sets:\n",
    "    dataset[\"S\"] = 0\n",
    "    dataset[\"C\"] = 0\n",
    "    dataset[\"Q\"] = 0\n",
    "\n",
    "    dataset.loc[dataset.Pclass == 1, \"S\"] = 1\n",
    "    dataset.loc[dataset.Pclass == 2, \"C\"] = 1\n",
    "    dataset.loc[dataset.Pclass == 3, \"Q\"] = 1\n",
    "    dataset.head()\n",
    "    dataset.drop(\"Embarked\", axis = 1, inplace = True) # removing embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "# creating a KNN imputer to fill in the missing age values\n",
    "knnImputer = KNNImputer(n_neighbors = 5)\n",
    "\n",
    "trainingSetResult = knnImputer.fit_transform(training)\n",
    "testSetResult = knnImputer.fit_transform(test)\n",
    "\n",
    "# converting the NP array returned from the KNN back into a Dataframe\n",
    "full_sets[0] = pd.DataFrame(data = trainingSetResult, columns = training.columns)\n",
    "full_sets[1] = pd.DataFrame(data = testSetResult, columns = test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing Dimentions\n",
    "# simplying data by adding sibling and parent/child together and creating FamilySize\n",
    "for dataset in full_sets:\n",
    "    familyCount = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        siblingCount = dataset.at[index, 'SibSp']\n",
    "        parentAndChildCount = dataset.at[index, 'Parch']\n",
    "\n",
    "        familyCount.append(siblingCount + parentAndChildCount)\n",
    "\n",
    "    dataset.drop(\"SibSp\", axis = 1, inplace = True) # removing sibling count\n",
    "    dataset.drop(\"Parch\", axis = 1, inplace = True) # removing parent/child count\n",
    "    dataset[\"FamilySize\"] = familyCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset \n",
    "print(full_sets[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Train Dataset\n",
    "# Female vs male\n",
    "plt = full_sets[0].Sex.value_counts().sort_index().plot(kind='bar')\n",
    "plt.set_xlabel('Sex')\n",
    "plt.set_ylabel('Passenger count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = full_sets[0][['Sex', 'Survived']].groupby('Sex').mean().Survived.plot(kind='bar')\n",
    "plt.set_xlabel('Sex')\n",
    "plt.set_ylabel('Survival Probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Age histogram\n",
    "g = sns.FacetGrid(full_sets[0], col='Survived')\n",
    "g.map(plt.hist, 'Age', bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass and Age  \n",
    "grid = sns.FacetGrid(full_sets[0], col='Survived', row='Pclass', size=2.2, aspect=1.6)\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survived percentage for every Social Class\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.pointplot(x=full_sets[0][\"Pclass\"], y='Survived', data=full_sets[0], ax = plt.subplot(313))\n",
    "plt.xlabel(\"P Class\", fontsize=14)\n",
    "plt.ylabel('Survived Percentage', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survived density plot for Fare prices\n",
    "fig = plt.figure(figsize=(15,9),)\n",
    "ax=sns.kdeplot(full_sets[0].loc[(full_sets[0]['Survived'] == 0),'Fare'] , color='black',shade=True,label='not survived')\n",
    "ax=sns.kdeplot(full_sets[0].loc[(full_sets[0]['Survived'] == 1),'Fare'] , color='g',shade=True, label='survived')\n",
    "plt.title('Fare Distribution Survived vs Non Survived', fontsize = 25, pad = 40)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\n",
    "plt.xlabel(\"Fare\", fontsize = 15, labelpad = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family Size histogram\n",
    "plt = full_sets[0].FamilySize.value_counts().sort_index().plot(kind='bar')\n",
    "plt.set_xlabel('SibSp')\n",
    "plt.set_ylabel('Passenger count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family Size Survived probability histogram\n",
    "plt = full_sets[0][['FamilySize', 'Survived']].groupby('FamilySize').mean().Survived.plot(kind='bar')\n",
    "plt.set_xlabel('FamilySize')\n",
    "plt.set_ylabel('Survival Probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Between features\n",
    "import matplotlib.pyplot as plt\n",
    "corr_matrix = full_sets[0].corr()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(data = corr_matrix,cmap='CMRmap', annot=True, linewidths=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning \n",
    "# Pclass Encoding\n",
    "for dataset in full_sets:\n",
    "    dataset[\"A_Class\"] = 0\n",
    "    dataset[\"B_Class\"] = 0\n",
    "    dataset[\"C_Class\"] = 0\n",
    "\n",
    "    dataset.loc[dataset.Pclass == 1, \"A_Class\"] = 1\n",
    "    dataset.loc[dataset.Pclass == 2, \"B_Class\"] = 1\n",
    "    dataset.loc[dataset.Pclass == 3, \"C_Class\"] = 1\n",
    "    dataset.head()\n",
    "    dataset.drop(\"Pclass\", axis = 1, inplace = True) # removing Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max normalization of age and fare\n",
    "for dataset in full_sets:\n",
    "    dataset['Age'] = MinMaxScaler().fit_transform(np.array(dataset['Age']).reshape(-1,1))\n",
    "    dataset['Fare'] = MinMaxScaler().fit_transform(np.array(dataset['Fare']).reshape(-1,1))\n",
    "    dataset['FamilySize'] = MinMaxScaler().fit_transform(np.array(dataset['FamilySize']).reshape(-1,1))\n",
    "    \n",
    "features = [\"Sex\", \"Age\", \"Fare\", \"A_Class\", \"B_Class\", \"C_Class\", \"FamilySize\", \"S\", \"C\", \"Q\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Models\n",
    "# ANN (Artificial Neural Network)\n",
    "\n",
    "dfMainX = full_sets[0][features] \n",
    "dfMainY = full_sets[0][\"Survived\"] # labels\n",
    "\n",
    "# Split Set into a training set and test set  (using 75% as training set and 25% as test set)\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfMainX, dfMainY, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the datasets to be readable by tensorflow\n",
    "x_train = tf.convert_to_tensor(x_train)\n",
    "x_test = tf.convert_to_tensor(x_test)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Artifial neural network\n",
    "\n",
    "ANN = tf.keras.Sequential([  # feed-forwards NN\n",
    "    tf.keras.layers.Flatten(input_shape=(10,)),# input layer\n",
    "    tf.keras.layers.Dense(128, activation='elu'), \n",
    "    tf.keras.layers.Dense(64, activation='elu'),\n",
    "    tf.keras.layers.Dense(32, activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(128, activation='elu'),\n",
    "    tf.keras.layers.Dense(64, activation='elu'),\n",
    "    tf.keras.layers.Dense(32, activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\") #  output layer with sigmoid\n",
    "])\n",
    "ANN.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=[\"binary_accuracy\"])\n",
    "ANN.fit(x_train, y_train, epochs= 100)\n",
    "# testing the ANN on the test set\n",
    "val_loss, val_acc = ANN.evaluate(x_test,y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using artificial neural network\n",
    "\n",
    "dfToPredict = full_sets[1][features] \n",
    "predictionFromANN = ANN.predict(dfToPredict)\n",
    "n = len(predictionFromANN) # converting to 0 and 1 died and survived \n",
    "survivedPredictions = []\n",
    "for i in range(0,n):\n",
    "    if(predictionFromANN[i][0] > 0.5 ):\n",
    "        survivedPredictions.append(1)\n",
    "    else:\n",
    "        survivedPredictions.append(0)\n",
    "ANNsubmission = submission.copy(deep=True)\n",
    "ANNsubmission.insert(1, \"Survived\",survivedPredictions , True)\n",
    "ANNsubmission.to_csv('results/ANNSubmission.csv', index=False) # Save as csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfMainX, dfMainY, test_size=0.1, random_state=2) # splitting the dataset\n",
    "# Finding the best parameters \n",
    "# create the grid\n",
    "n_estimators = [10, 19, 20, 24, 25,26,27]\n",
    "max_depth = [4, 5, 6, 7]\n",
    "leaf_samples = [1, 2, 3, 4]\n",
    "grid = dict(n_estimators=n_estimators, max_depth=max_depth, min_samples_leaf=leaf_samples)\n",
    "\n",
    "RF = RandomForestClassifier(random_state=2)# create default RL model\n",
    "\n",
    "# search the grid\n",
    "grid = GridSearchCV(estimator=RF, \n",
    "                    param_grid=grid,\n",
    "                    cv=3,\n",
    "                    verbose=2,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "print(grid_result.best_score_, \"Was achieved using:\",grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best Random Forest Classifier Parameters\n",
    "RF = grid_result.best_estimator_\n",
    "RF.fit(x_train, y_train)\n",
    "y_pred = RF.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the result on the test set\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using RF classifier \n",
    "\n",
    "dfToPredict = full_sets[1][features] \n",
    "predictionFromRF = RF.predict(dfToPredict)\n",
    "predictionFromRF = [int(x) for x in predictionFromRF] # converting to int type\n",
    "RFsubmission = submission.copy(deep=True)\n",
    "RFsubmission.insert(1, \"Survived\",predictionFromRF , True)\n",
    "RFsubmission.to_csv('results/RFSubmission.csv', index=False) # Save as csv for submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "X_train = full_sets[0].drop(\"Survived\", axis = 1)\n",
    "y_train = full_sets[0][\"Survived\"]\n",
    "X_test = full_sets[1]\n",
    "\n",
    "logRegModel = LogisticRegression(penalty = 'l2', solver = \"sag\", random_state = 0)\n",
    "logRegModel.fit(X_train, y_train)\n",
    "\n",
    "#Testpredictions = logRegModel.predict(x_test)\n",
    "# Analyse the result on the test set\n",
    "#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, Testpredictions)))\n",
    "\n",
    "\n",
    "predictions = logRegModel.predict(X_test)\n",
    "predictions_ints = predictions.astype(int)\n",
    "\n",
    "submission[\"Survived\"] = predictions_ints.tolist()\n",
    "\n",
    "submission.to_csv('results/LogisticRegressionSubmission.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "#Testpredictions = svc.predict(x_test)\n",
    "# Analyse the result on the test set\n",
    "#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, Testpredictions)))\n",
    "\n",
    "Y_pred = svc.predict(X_test)\n",
    "\n",
    "Y_pred_ints = Y_pred.astype(int)\n",
    "submission[\"Survived\"] = Y_pred_ints.tolist()\n",
    "\n",
    "submission.to_csv('results/SVMSubmission.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to be used to eveluate models\n",
    "def cross_validate(estimator, train, validation):\n",
    "    X_train = train[0]\n",
    "    Y_train = train[1]\n",
    "    X_val = validation[0]\n",
    "    Y_val = validation[1]\n",
    "    t_predictions = classifier.predict(X_train)\n",
    "    t_accuracy = accuracy_score(t_predictions, Y_train)\n",
    "    t_recall = recall_score(t_predictions, Y_train)\n",
    "    t_precision = precision_score(t_predictions, Y_train)\n",
    "\n",
    "    val_predictions = classifier.predict(X_val)\n",
    "    val_accuracy = accuracy_score(val_predictions, Y_val)\n",
    "    val_recall = recall_score(val_predictions, Y_val)\n",
    "    val_precision = precision_score(val_predictions, Y_val)\n",
    "\n",
    "    print('Model stats')\n",
    "    print('Accuracy  Train: %.2f, Validation: %.2f' % (t_accuracy, val_accuracy))\n",
    "    print('Recall    Train: %.2f, Validation: %.2f' % (t_recall, val_recall))\n",
    "    print('Precision Train: %.2f, Validation: %.2f' % (t_precision, val_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "X = full_sets[0][features] \n",
    "Y = full_sets[0][\"Survived\"] \n",
    "\n",
    "# splitting data \n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "X_train1, X_train2, Y_train1, Y_train2 = train_test_split(X_train, Y_train, test_size=0.3, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train2, Y_train2)\n",
    "\n",
    "# information\n",
    "print('30% of train data')\n",
    "cross_validate(classifier, (X_train, Y_train), (X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.partial_fit(X_train1, Y_train1)\n",
    "print('70% of train data')\n",
    "cross_validate(classifier, (X_train, Y_train), (X_val, Y_val))\n",
    "\n",
    "print(\"note: improved results with second fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting using Gaussian Naive Bayes\n",
    "dfToPredict = full_sets[1][features] \n",
    "test_predictions = classifier.predict(dfToPredict)\n",
    "NBSubmission = submission.copy(deep=True)\n",
    "NBSubmission.drop(\"Survived\", inplace = True, axis = 1)\n",
    "NBSubmission.insert(1, \"Survived\",test_predictions.astype('int'), True)\n",
    "NBSubmission.to_csv('results/submissionNB.csv', index=False) # Save as csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Algorithm using k = 5 or 6 or 29\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "n5 = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
    "y5 = n5.predict(X_test)\n",
    "cm5 = metrics.confusion_matrix(y_test, y5)\n",
    "print('Test set Accuracy of K=5:', metrics.accuracy_score(y_test, y5))\n",
    "print(cm5)\n",
    "\n",
    "n6 = KNeighborsClassifier(n_neighbors=6).fit(X_train, y_train)\n",
    "y6 = n6.predict(X_test)\n",
    "cm6 = metrics.confusion_matrix(y_test, y6)\n",
    "print('Test set Accuracy of K=6: ', metrics.accuracy_score(y_test, y6))\n",
    "print(cm6)\n",
    "\n",
    "\n",
    "n29 = KNeighborsClassifier(n_neighbors=29).fit(X_train, y_train)\n",
    "y29 = n29.predict(X_test)\n",
    "cm29 = metrics.confusion_matrix(y_test, y29)\n",
    "print('Test set Accuracy of K=29 : ', metrics.accuracy_score(y_test, y29))\n",
    "print(cm29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"datasets/test.csv\")\n",
    "df_test_copy = df_test.copy()\n",
    "\n",
    "# so far, 5 is best\n",
    "X_submit = np.array(dfToPredict)\n",
    "y_submit = n5.predict(X_submit)\n",
    "\n",
    "submit = df_test_copy[['PassengerId']].copy()\n",
    "submit['Survived'] = y_submit\n",
    "\n",
    "submission['Survived'] = y_submit.astype('int')\n",
    "submission.to_csv('results/submissionKNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
